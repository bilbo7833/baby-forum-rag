{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from llama_index.core.node_parser.text import SentenceSplitter\n",
    "\n",
    "EMBEDDING_MODEL = 'aari1995/German_Semantic_STS_V2'\n",
    "\n",
    "def convert_html_to_text(html_string):\n",
    "    soup = BeautifulSoup(html_string, 'html.parser')\n",
    "    return soup.get_text().strip().replace('\\r','').replace('\\n\\n','\\n')\n",
    "\n",
    "def save_file(file, text):\n",
    "    f = open(file, \"w\")\n",
    "    f.write(text)\n",
    "    f.close()\n",
    "\n",
    "def token_length_function(tokenizer, text_input):\n",
    "  return len(tokenizer.encode(text_input, add_special_tokens=False))\n",
    "\n",
    "file_path = '../../scraper/forum_posts.json'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# The entire json payload is too large to be processed in memory. We take just the first 100 forum topics.\n",
    "data = data[0:1000]\n",
    "\n",
    "# Format the html inside the dictionary and remove trailing whitespaces\n",
    "for item in data:\n",
    "    item['title'] = convert_html_to_text(item['title'])\n",
    "    for post in item['posts']:\n",
    "        post['post'] = convert_html_to_text(post['post'])\n",
    "\n",
    "# # Write the forum topics as cursive conversations for chunking.\n",
    "# output_text = \"\"\n",
    "# for item in data:\n",
    "#     if output_text != \"\":\n",
    "#         output_text += \"\\n\\n\" \n",
    "#     output_text += \"Thema:\" + item[\"title\"] + \"\\n\"\n",
    "#     for post in item['posts']:\n",
    "#         poster = post['poster'] if post['poster'] is not None else 'Unknown'\n",
    "#         output_text += poster + \": \" + post['post'] + \"\\n\\n\"\n",
    "# print(output_text)\n",
    "# Write the cursive text to file\n",
    "# save_file(\"forum_posts_100.txt\", output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(data, tokenizer):\n",
    "    text_splitter = SentenceSplitter(\n",
    "        separator=\" \",\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=50,\n",
    "        paragraph_separator=\"\\n\\n\",\n",
    "        tokenizer=tokenizer.encode,\n",
    "    )\n",
    "    chunks = []\n",
    "    for item in data:\n",
    "        text = \"Thema:\" + item[\"title\"] + \"\\n\"\n",
    "        for post in item['posts']:\n",
    "            poster = post['poster'] if post['poster'] is not None else 'Unknown'\n",
    "            text += poster + \": \" + post['post'] + \"\\n\\n\"\n",
    "        item_chunks = text_splitter.split_text(text)\n",
    "        chunks.extend(item_chunks)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1116 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
    "chunks = get_chunks(data, tokenizer)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max chunk size: 501 and Min chunk size: 14\n"
     ]
    }
   ],
   "source": [
    "token_lengths = [token_length_function(tokenizer, c) for c in chunks]\n",
    "print(f\"Max chunk size: {max(token_lengths)} and Min chunk size: {min(token_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.environ.get('OPENAI_API_KEY'))\n",
    "# print(os.environ.get('ANTHROPIC_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating LlamaIndex Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "# Load data\n",
    "documents = [Document(text=chunk) for chunk in chunks]\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SentenceSplitter(\n",
    "        separator=\" \",\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=50,\n",
    "        paragraph_separator=\"\\n\\n\",\n",
    "        tokenizer=tokenizer.encode,\n",
    "    )\n",
    "# Nodes from Documents\n",
    "nodes = text_splitter.get_nodes_from_documents(documents)\n",
    "print(f\"We have parsed {len(nodes)} nodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start Docker Service\n",
    "2. Start Docker configuration in `docker-compose.yaml` with `docker compose up`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core import StorageContext, VectorStoreIndex, Settings\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Anthropic().tokenizer\n",
    "# llm = Anthropic(model=\"claude-3-haiku-20240307\")\n",
    "embedding_model = HuggingFaceEmbedding(model_name=EMBEDDING_MODEL)\n",
    "# Settings.tokenizer = tokenizer\n",
    "# Settings.llm = llm\n",
    "Settings.embed_model = embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"BabyForum\"\n",
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "if client.schema.exists(INDEX_NAME):\n",
    "    client.schema.delete_class(INDEX_NAME)\n",
    "print(client.get_meta())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = WeaviateVectorStore(weaviate_client = client, embed_model= embedding_model, index_name=INDEX_NAME)\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "index = VectorStoreIndex(nodes, storage_context = storage_context, embed_model=embedding_model, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.storage_context.persist(persist_dir='weaviate-index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import get_response_synthesizer\n",
    "\n",
    "# retriever = VectorIndexRetriever(\n",
    "#     index=index,\n",
    "#     similarity_top_k=10,\n",
    "# )\n",
    "# response_synthesizer = get_response_synthesizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine = RetrieverQueryEngine(\n",
    "#     retriever=retriever,\n",
    "#     response_synthesizer=response_synthesizer,\n",
    "#     node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.8)],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer1 = query_engine.query(\"Kind wacht stündlich auf\")\n",
    "# save_file('sources.txt', str(answer1.source_nodes))\n",
    "# for n in answer1.source_nodes:\n",
    "#     print(f\"Node {n.node_id} with similarity score {n.score}:\\n{n.text}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer2 = query_engine.query(\"Wer kümmert sich um das Kind nachts?\")\n",
    "# for n in answer2.source_nodes:\n",
    "#     print(f\"Node {n.node_id} with similarity score {n.score}:\\n{n.text}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer3 = query_engine.query(\"Wann darf man mit kind ins Tropical Island?\")\n",
    "# for n in answer3.source_nodes:\n",
    "#     print(f\"Node {n.node_id} with similarity score {n.score}:\\n{n.text}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.environ.get('OPENAI_API_KEY'))\n",
    "# print(os.environ.get('ANTHROPIC_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if client.collections.exists(\"ForumPost\"):\n",
    "#     client.collections.delete(\"ForumPost\")\n",
    "# client.collections.create(\n",
    "#     \"ForumPost\",\n",
    "#     vectorizer_config=Configure.Vectorizer.text2vec_transformers(),\n",
    "#     generative_config=Configure.Generative.openai(model='gpt-3.5-turbo'),\n",
    "#     properties=[\n",
    "#         Property(name=\"body\", data_type=DataType.TEXT),\n",
    "#     ]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post1 = {'body': \"Das ist ein langes Post über weinenende Kinder\"}\n",
    "# post2 = {'body': \"Mein Baby isst nichts mehr tagsüber\"}\n",
    "# post3 = {'body': \"Kind wacht Nachts weinend auf und schreit\"}\n",
    "# posts = client.collections.get(\"ForumPost\")\n",
    "# posts.data.insert(post1)\n",
    "# posts.data.insert_many([post2, post3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts = client.collections.get(\"ForumPost\")\n",
    "# for item in posts.iterator(include_vector=True):\n",
    "#     print(item.properties)\n",
    "#     print(item.vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = posts.query.near_text(\n",
    "#         query=\"Baby weint\",\n",
    "#         return_metadata=wvc.query.MetadataQuery(distance=True),\n",
    "#         limit=2\n",
    "#     )\n",
    "\n",
    "# for o in response.objects:\n",
    "#     print(o.properties)\n",
    "#     print(o.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = posts.query.hybrid(\n",
    "#         query=\"Baby weint\",\n",
    "#         return_metadata=wvc.query.MetadataQuery(distance=True, score=True, explain_score=True),\n",
    "#         alpha=0.75,\n",
    "#         limit=3\n",
    "#     )\n",
    "\n",
    "# for o in response.objects:\n",
    "#     print(o.properties)\n",
    "#     print(o.metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbia-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
