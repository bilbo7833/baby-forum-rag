{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_html_to_text(html_string):\n",
    "    soup = BeautifulSoup(html_string, 'html.parser')\n",
    "    return soup.get_text().strip().replace('\\r','').replace('\\n\\n','\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the JSON file with the HTML contents of the forum conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../../scraper/forum_posts.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "# data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire json payload is too large to be processed in memory. We take just the first 100 forum topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the html inside the dictionary and remove trailing whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    item['title'] = convert_html_to_text(item['title'])\n",
    "    for post in item['posts']:\n",
    "        post['post'] = convert_html_to_text(post['post'])\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the forum topics as cursive conversations for chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = \"\"\n",
    "for item in data:\n",
    "    if output_text != \"\":\n",
    "        output_text += \"\\n\\n\" \n",
    "    output_text += \"Thema:\" + item[\"title\"] + \"\\n\"\n",
    "    for post in item['posts']:\n",
    "        poster = post['poster'] if post['poster'] is not None else 'Unknown'\n",
    "        output_text += poster + \": \" + post['post'] + \"\\n\\n\"\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the cursive text to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"forum_posts.txt\", \"w\")\n",
    "f.write(output_text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the text into chunks with the tokenizer from the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDING_MODEL = 'aari1995/German_Semantic_STS_V2'\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "chunk_size = model.get_max_seq_length()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the tokenizer from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
    "\n",
    "def token_length_function(text_input):\n",
    "  return len(tokenizer.encode(text_input, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for better specialized tokenixer for German language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from somajo import SoMaJo\n",
    "# from itertools import chain\n",
    "# tokenizer = SoMaJo(\"de_CMC\", split_camel_case=True)\n",
    "# sentences = tokenizer.tokenize_text([\"\"\"Ca. 90min mit newmotion geladen, weil ich mit Maingau/EinfachStromLaden keine Verbindung 체ber die App bekam. \n",
    "# S채ule hat keinen RFID-Leser usw.\n",
    "# 2. Buchse seit 2 Tagen mit Kommunalfahrzeug/EWV blockiert. \"\"\"])\n",
    "# # for sentence in sentences:\n",
    "# #     for token in sentence:\n",
    "# #         print(token.text)\n",
    "# #     print()\n",
    "# len(list(x.text for x in chain.from_iterable(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the text by delimiting forum topics (\\n\\n\\n), then individual posts (\\n\\n) and then regular new lines (\\n). Use the `token_length_function` to keep the chunk size limited to the model chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = 50,\n",
    "    length_function = token_length_function,\n",
    "    separators=[\"\\n\\n\\n\",\"\\n\\n\", \"\\n\"])\n",
    "chunks = splitter.split_text(output_text)\n",
    "# for i, chunk in enumerate(chunks):\n",
    "#     print(f\"Chunk #{i} with length {len(chunk)} characters and {token_length_function(chunk)} tokens: \\n{chunk}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of chunk token sizes\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist([token_length_function(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "model_kwargs = {'device':'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the custom embedding model\n",
    "embedding_model_wrapper = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "embedding_output = model.encode(chunks, normalize_embeddings=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(embedding_output)} embeddings of size {len(embedding_output[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def print_matches(matches):\n",
    "    for p in matches:\n",
    "        print(f\"\\n\\nMatch with similarity {p[1]}:\\n{p[0].page_content}\")\n",
    "\n",
    "text_embedding_pairs = zip(chunks, embedding_output)\n",
    "vector_store = FAISS.from_embeddings(text_embedding_pairs, embedding_model_wrapper)\n",
    "vector_store.save_local(\"forum_index\")\n",
    "answer1 = vector_store.similarity_search_with_score(\"Kind wacht st체ndlich auf\")\n",
    "print_matches(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer2 = vector_store.similarity_search_with_score(\"Was sind Hexenstunden?\")\n",
    "print_matches(answer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer3 = vector_store.similarity_search_with_score(\"Wer k체mmert sich um das Kind nachts?\")\n",
    "print_matches(answer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer4 = vector_store.similarity_search_with_score(\"Wann darf man mit Kind ins Tropical Island?\")\n",
    "print_matches(answer4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbia-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
