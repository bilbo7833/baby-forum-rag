{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name aari1995/German_Semantic_STS_V2. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import weaviate\n",
    "import tiktoken\n",
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core import StorageContext, VectorStoreIndex, get_response_synthesizer, Settings\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"Du bist ein hilfreicher deutscher Assistent f√ºr junge Eltern, der Informationen aus Foren sammelt, um Eltern bei der Beantwortung von Fragen √ºber ihre Kinder zu helfen.\n",
    "\n",
    "            Verwende die folgenden Konversationen aus Elternforen, um den Nutzer √ºber ein bestimmtes Thema zu helfen. Jede Zeile beginnt mit dem Namen des Benutzers, gefolgt von \":\" und dann dem Kommentar, zum Beispiel so: \"John: Bei mir ist es genauso.\"\n",
    "            Verschiedene Konversationen k√∂nnen sich auf dasselbe Thema beziehen.\n",
    "            <conversations>\n",
    "            {context_str}\n",
    "            </conversations>\n",
    "            \n",
    "            Wenn du die Unterhaltungen im Forum zitierst, gib bitte den Benutzernamen in deiner Antwort an. Hier sind Beispielw in <example> Tags:\n",
    "            <example>\n",
    "            Viele Nutzer sagen dass es normal ist Kinder nachts zu stillen. Cari234 sagt, z.B, dass sie t√§glich √ºberfordert ist. Lomo2 hat gleiche Erfahrungen.\n",
    "            </example>\n",
    "            <example>\n",
    "            Die Konversationen beinhalten keine Hinweise ob es normal ist, dass Kinder nachts Wachfenster haben.\n",
    "            </example>\n",
    "\n",
    "            Hier ist deine Aufgabe: Welche relevanten Informationen kannst du aus den oben genannten Gespr√§chen zu diesem Thema entnehmen? Das Thema ist unten in <question>-Tags:\n",
    "            <question>\n",
    "            {query_str}\n",
    "            </question>\n",
    "\n",
    "            Um deine Aufgabe zu erledigen, gehe die folgenden Schritte durch:\n",
    "            1. Erstelle eine umfassende Zusammenfassung f√ºr jede Konversation in <conversation> Tags oben. Die Zusammenfassungen sollte alle wichtigen Punkte und Hauptgedanken des Originaltextes die sich auf diesem Aufgabe Thema beziehen abdecken und gleichzeitig die Informationen in einem pr√§gnanten und leicht verst√§ndlichen Format zusammenfassen.\n",
    "            Achte bitte darauf, dass die Zusammenfassung die Usernamen, relevante Details und Beispiele enth√§lt, die die Hauptgedanken unterst√ºtzen, und vermeide unn√∂tige Informationen oder Wiederholungen.\n",
    "            2. Erledige deine Aufgabe auf der Grundlage der Zusammenfassungen von Schritt 1. F√ºge alle relevanten Informationen, Details und Beispiele ein die aus der Zusammenfassungen rauskommen und sich auf diesem Thema beziehen. Behalte die Antwort auf maximal 5 S√§tze.\n",
    "\n",
    "            Wenn du die Aufgabe erledigst, nenn bitte konkrete Beispiele und Tipps aus den Forendiskussionen und verallgemeinere die Details nicht. Wenn du die Antwort nicht wei√üt, sag einfach, dass du es nicht wei√üt.\n",
    "            Fang deine Antwort mit \"Das sagen andere Nutzer dazu:\" an. Danach, gib die Zusammenfassungen von Schritt 1 aus als Bulletpoint-Liste aus. F√ºr jede Konversation soll es ein Bulletpoint geben. Gib dann deine zusammenfassende Antwort gem√§√ü Schritt 2 in eine neue Zeile ein.\n",
    "            Hier sind Beispiele wie deine Antworten formattiert werden sollen, in <answer-example>-Tags:\n",
    "\n",
    "            <answer-example>\n",
    "            Das sagen andere Nutzer dazu:\n",
    "            - jomda erkl√§rt, dass Babys manchmal schreien, wenn √§ltere Geschwister versorgt werden m√ºssen oder wenn das Baby nachts wach wird, weil der Betreuer kurz etwas erledigen muss.\n",
    "            - Mami83 berichtet, dass ihr Baby auch Phasen hatte, in denen es sehr unruhig war und h√§ufig nachts aufwachte. Caro34 best√§tigt.\n",
    "\n",
    "            Das sagen die Nutzer zusammengefasst: H√§ufiges n√§chtliches Aufwachen und Unruhe sind bei Babys oft normal und h√§ngen mit deren Entwicklung und Bed√ºrfnissen zusammen. Die Situation kann vor√ºbergehend sehr herausfordernd sein, aber die Eltern m√ºssen durchhalten, da es mit der Zeit wieder besser wird.\n",
    "            </answer-example>\n",
    "            <answer-example>\n",
    "            Das sagen andere Nutzer dazu:\n",
    "            - bilbo45 sagt, dass solche schwierigen Phasen normal sind und immer wieder kommen k√∂nnen, da sich Babys st√§ndig weiterentwickeln. Sie ermutigt, durchzuhalten, da es mit der Zeit besser wird.\n",
    "            - Micebwn beschreibt, dass ihr 9 Monate altes Baby seit Tagen nachts weinend aufwacht.\n",
    "\n",
    "            Aus den Gespr√§chen geht hervor, dass bei den meisten Kindern dieser spezielle Test mit schwarzen und wei√üen Punkten, bei denen das Kind etwas Bestimmtes erkennen und zeigen soll, im Alter von 11 Monaten noch nicht funktioniert. Die Eltern berichten, dass ihre Kinder stattdessen eher versuchen, mit den Fingern in den Mund der √Ñrzte zu kommen oder generell eher an der Untersuchung interessiert sind als an dem Test.\n",
    "            </answer-example>    \n",
    "\n",
    "            Erledige jetzt bitte deine Aufgabe bez√ºglich des Themas von oben.\n",
    "            \"\"\"\n",
    "EMBEDDING_MODEL = 'aari1995/German_Semantic_STS_V2'\n",
    "# MAX_TOKENS = 4000\n",
    "\n",
    "groq_api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "llm =  Groq(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            api_key=groq_api_key,\n",
    "            # max_tokens=MAX_TOKENS\n",
    "        )\n",
    "# llm =  Groq(\n",
    "#             model=\"mixtral-8x7b-32768\",\n",
    "#             api_key=groq_api_key,\n",
    "#         )\n",
    "# llm = Anthropic(model=\"claude-3-haiku-20240307\")\n",
    "embedding_model = HuggingFaceEmbedding(model_name=EMBEDDING_MODEL)\n",
    "# Settings.tokenizer = tokenizer\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_model\n",
    "# token_counter = TokenCountingHandler(\n",
    "#     tokenizer=tiktoken.encoding_for_model(\"llama3-70b-8192\").encode\n",
    "# )\n",
    "# Settings.callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "vector_store = WeaviateVectorStore(weaviate_client = client, embed_model=embedding_model, index_name=\"BabyForum\")\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, storage_context=storage_context)\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    "    # filters=filters,\n",
    ")\n",
    "response_synthesizer = get_response_synthesizer(llm = llm, response_mode=\"simple_summarize\")\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.75)],\n",
    ")\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(PROMPT_TEMPLATE)\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Calculated available context size -1200 was not non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m answer1 \u001b[38;5;241m=\u001b[39m query_engine\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKind hat ein Stein verschluckt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/urbia-rag/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/miniconda3/envs/urbia-rag/lib/python3.12/site-packages/llama_index/core/base/base_query_engine.py:53\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     52\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 53\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query(str_or_query_bundle)\n\u001b[1;32m     54\u001b[0m dispatch_event(QueryEndEvent())\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[0;32m~/miniconda3/envs/urbia-rag/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/miniconda3/envs/urbia-rag/lib/python3.12/site-packages/llama_index/core/query_engine/retriever_query_engine.py:190\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    187\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    188\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m    189\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[0;32m--> 190\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_synthesizer\u001b[38;5;241m.\u001b[39msynthesize(\n\u001b[1;32m    191\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery_bundle,\n\u001b[1;32m    192\u001b[0m         nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/urbia-rag/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/miniconda3/envs/urbia-rag/lib/python3.12/site-packages/llama_index/core/response_synthesizers/base.py:241\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m     query \u001b[38;5;241m=\u001b[39m QueryBundle(query_str\u001b[38;5;241m=\u001b[39mquery)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    238\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE,\n\u001b[1;32m    239\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query\u001b[38;5;241m.\u001b[39mquery_str},\n\u001b[1;32m    240\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 241\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[1;32m    242\u001b[0m         query_str\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39mquery_str,\n\u001b[1;32m    243\u001b[0m         text_chunks\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    244\u001b[0m             n\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mLLM) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[1;32m    245\u001b[0m         ],\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    249\u001b[0m     additional_source_nodes \u001b[38;5;241m=\u001b[39m additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    250\u001b[0m     source_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nodes) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m~/miniconda3/envs/urbia-rag/lib/python3.12/site-packages/llama_index/core/response_synthesizers/simple_summarize.py:90\u001b[0m, in \u001b[0;36mSimpleSummarize.get_response\u001b[0;34m(self, query_str, text_chunks, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m text_qa_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_qa_template\u001b[38;5;241m.\u001b[39mpartial_format(query_str\u001b[38;5;241m=\u001b[39mquery_str)\n\u001b[1;32m     89\u001b[0m single_text_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text_chunks)\n\u001b[0;32m---> 90\u001b[0m truncated_chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt_helper\u001b[38;5;241m.\u001b[39mtruncate(\n\u001b[1;32m     91\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mtext_qa_template,\n\u001b[1;32m     92\u001b[0m     text_chunks\u001b[38;5;241m=\u001b[39m[single_text_chunk],\n\u001b[1;32m     93\u001b[0m )\n\u001b[1;32m     95\u001b[0m response: RESPONSE_TEXT_TYPE\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n",
      "File \u001b[0;32m~/miniconda3/envs/urbia-rag/lib/python3.12/site-packages/llama_index/core/indices/prompt_helper.py:255\u001b[0m, in \u001b[0;36mPromptHelper.truncate\u001b[0;34m(self, prompt, text_chunks, padding, llm)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtruncate\u001b[39m(\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    249\u001b[0m     prompt: BasePromptTemplate,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m     llm: Optional[LLM] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    253\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    254\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Truncate text chunks to fit available context window.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m     text_splitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_text_splitter_given_prompt(\n\u001b[1;32m    256\u001b[0m         prompt,\n\u001b[1;32m    257\u001b[0m         num_chunks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text_chunks),\n\u001b[1;32m    258\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m    259\u001b[0m         llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m    260\u001b[0m     )\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [truncate_text(chunk, text_splitter) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks]\n",
      "File \u001b[0;32m~/miniconda3/envs/urbia-rag/lib/python3.12/site-packages/llama_index/core/indices/prompt_helper.py:234\u001b[0m, in \u001b[0;36mPromptHelper.get_text_splitter_given_prompt\u001b[0;34m(self, prompt, num_chunks, padding, llm)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_text_splitter_given_prompt\u001b[39m(\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    226\u001b[0m     prompt: BasePromptTemplate,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m     llm: Optional[LLM] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TokenTextSplitter:\n\u001b[1;32m    231\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get text splitter configured to maximally pack available context window,\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m    taking into account of given prompt, and desired number of chunks.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_available_chunk_size(\n\u001b[1;32m    235\u001b[0m         prompt, num_chunks, padding\u001b[38;5;241m=\u001b[39mpadding, llm\u001b[38;5;241m=\u001b[39mllm\n\u001b[1;32m    236\u001b[0m     )\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not positive.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/urbia-rag/lib/python3.12/site-packages/llama_index/core/indices/prompt_helper.py:218\u001b[0m, in \u001b[0;36mPromptHelper._get_available_chunk_size\u001b[0;34m(self, prompt, num_chunks, padding, llm)\u001b[0m\n\u001b[1;32m    215\u001b[0m     prompt_str \u001b[38;5;241m=\u001b[39m get_empty_prompt_txt(prompt)\n\u001b[1;32m    216\u001b[0m     num_prompt_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_counter\u001b[38;5;241m.\u001b[39mget_string_tokens(prompt_str)\n\u001b[0;32m--> 218\u001b[0m available_context_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_available_context_size(num_prompt_tokens)\n\u001b[1;32m    219\u001b[0m result \u001b[38;5;241m=\u001b[39m available_context_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_chunks \u001b[38;5;241m-\u001b[39m padding\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/urbia-rag/lib/python3.12/site-packages/llama_index/core/indices/prompt_helper.py:150\u001b[0m, in \u001b[0;36mPromptHelper._get_available_context_size\u001b[0;34m(self, num_prompt_tokens)\u001b[0m\n\u001b[1;32m    148\u001b[0m context_size_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_window \u001b[38;5;241m-\u001b[39m num_prompt_tokens \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_output\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context_size_tokens \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculated available context size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_size_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not non-negative.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m     )\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m context_size_tokens\n",
      "\u001b[0;31mValueError\u001b[0m: Calculated available context size -1200 was not non-negative."
     ]
    }
   ],
   "source": [
    "answer1 = query_engine.query(\"Kind hat ein Stein verschluckt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source #0 has score 0.80401653:\n",
      "lizzywithD: danke dir! das mit dem im eigenen zimmer schlafen √ºberlege ich mir st√§ndig aber ich kann\n",
      "Source #1 has score 0.79711473:\n",
      "SteMatz: Schlichtweg ... JA. Meine Maus schl√§ft auch durch, leider sehr unruhig und ich werde bei je\n",
      "Source #2 has score 0.79370987:\n",
      "Mami823: War bei uns genauso. Als er ca. 3,5 Monate alt war, habe ich mein Baby nicht wiedererkannt.\n",
      "Source #3 has score 0.79095107:\n",
      "Sonnenschein729: Hallo üôãüèº‚Äç‚ôÄÔ∏è mein Sohn wird n√§chste Woche erst f√ºnf Monate alt. Liest sich als w√§re \n",
      "Source #4 has score 0.79071355:\n",
      "Thema:ist das noch normal?\n",
      "lizzywithD: hallo community\n",
      "beitrag nr. 4 in k√ºrzerer zeit. \n",
      "thema: babys\n",
      "Source #5 has score 0.78898525:\n",
      "Thema:Baby macht in der Nacht Situps\n",
      "LiNo33: Hallo zusammen,\n",
      "mein kleiner schl√§ft seit einigen Woche\n",
      "Source #6 has score 0.78773314:\n",
      "Thema:Baby 21 Wochen alt\n",
      "Vanni9: Hallo zusammen, \n",
      "Unser kleine ist heute 21 Wochen alt. Seit circa 2\n",
      "Source #7 has score 0.7872818699999999:\n",
      "Tanja.SoSo: Hall√∂chen und ein frohes neues Jahr euch allen! üçÄüéÜ\n",
      "Wahnsinn, wie schnell die Zeit doch r\n",
      "Source #8 has score 0.78485173:\n",
      "Jolana: Ich geh√∂r auch zur Kategorie \"klingt wie bei uns und normal\". Der Kleine (4,5 Monate) schl√§f\n",
      "Source #9 has score 0.784706:\n",
      "Thema:Baby sehr zappelig unruhig\n",
      "Nanus: Hallo muttis\n",
      "Meine tochter fast 5 moange alt ist sehr zappel\n",
      "Das sagen andere Nutzer dazu:\n",
      "\n",
      "* LizzywithD beschreibt, dass ihr Baby st√ºndlich aufwacht und sie sich Sorgen macht, ob ihr Baby eine Schlafst√∂rung hat.\n",
      "* Kuddelmuddel erz√§hlt, dass ihr Sohn Junior schnarchte und deshalb nicht im Familienbett schlafen konnte. Sie bemerkte, dass er besser schlief, wenn er nicht mehr neben ihnen lag.\n",
      "* Elly0109 teilt mit, dass ihre Tochter fr√ºher zwischen 3:30 und 4 Uhr aufwachte und dass Kinder unterschiedlich sind und sich √§ndern.\n",
      "* SteMatz sagt, dass ihr Kind auch unruhig schl√§ft und sie oft geweckt wird.\n",
      "* Blabliblu meint, dass man sich an den Schlafmangel gew√∂hnt und dass es wichtig ist, den richtigen Schlafzyklus zu finden.\n",
      "* Muriel erkl√§rt, dass es wichtig ist, den richtigen Schlafzyklus zu finden und dass die Hormone auch eine Rolle spielen.\n",
      "* Lattemachiatto teilt mit, dass sie m√ºde war, wenn ihr Baby weiter weg von ihr schlief und dass sie besser schlief, wenn das Baby direkt neben ihr lag.\n",
      "* Mami823 erz√§hlt, dass ihr Baby auch sehr unruhig war und dass sie es konsequent alle 2 Stunden zum Schlafen gebracht hat.\n",
      "* Sonnenschein729 beschreibt, dass ihr Sohn auch Probleme hatte, einzuschlafen, und dass er gerne sitzt und seine Bauchmuskeln testet.\n",
      "\n",
      "Das sagen die Nutzer zusammengefasst: Es ist normal, dass Babys st√ºndlich aufwachen und dass es wichtig ist, den richtigen Schlafzyklus zu finden. Die Eltern m√ºssen durchhalten, da es mit der Zeit wieder besser wird.\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for source in answer1.source_nodes:\n",
    "    print(f\"Source #{ count } has score {source.score}:\\n{source.text[:100]}\")\n",
    "    count += 1\n",
    "\n",
    "print(answer1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbia-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
